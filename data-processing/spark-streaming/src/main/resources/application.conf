# CityFlow Spark Streaming Jobs Configuration

cityflow {
  app {
    name = "CityFlow Data Processing"
    version = "1.0.0"
  }

  # Kafka Configuration
  kafka {
    bootstrap-servers = "localhost:9093"
    bootstrap-servers = ${?KAFKA_BOOTSTRAP_SERVERS}
    
    schema-registry-url = "http://localhost:8081"
    schema-registry-url = ${?SCHEMA_REGISTRY_URL}
    
    topics {
      traffic-reading = "traffic.reading.events"
      bus-location = "bus.location.events"
      incident = "incident.events"
      sensor-status = "sensor.status.events"
      bus-status = "bus.status.events"
      
      # Output topics
      enriched-incident = "incident.enriched.events"
      dead-letter-queue = "dlq.events"
    }
    
    consumer {
      group-id-prefix = "spark-streaming"
      auto-offset-reset = "latest"
      max-poll-records = 500
    }
  }

  # PostgreSQL Configuration
  postgresql {
    url = "jdbc:postgresql://localhost:5432/cityflow"
    url = ${?POSTGRES_URL}
    
    user = "postgres"
    user = ${?POSTGRES_USER}
    
    password = "postgres"
    password = ${?POSTGRES_PASSWORD}
    
    driver = "org.postgresql.Driver"
    
    connection-pool {
      max-size = 10
      min-size = 2
    }
  }

  # MongoDB Configuration
  mongodb {
    uri = "mongodb://localhost:27017/cityflow"
    uri = ${?MONGODB_URI}
    
    database = "cityflow"
    
    collections {
      traffic-readings = "traffic_readings"
      bus-positions = "bus_positions"
      incidents = "incidents"
    }
  }

  # Redis Configuration
  redis {
    host = "localhost"
    host = ${?REDIS_HOST}
    
    port = 6379
    port = ${?REDIS_PORT}
    
    database = 0
    
    ttl {
      traffic-cache = 300      # 5 minutes
      bus-position = 180       # 3 minutes
      analytics = 60           # 1 minute
    }
  }

  # Delta Lake Configuration
  delta-lake {
    base-path = "/tmp/cityflow/delta-lake"
    base-path = ${?DELTA_LAKE_PATH}
    
    checkpoint-location = "/tmp/cityflow/checkpoints"
    checkpoint-location = ${?CHECKPOINT_PATH}
    
    partitioning {
      traffic {
        columns = ["date", "hour"]
        pattern = "yyyy-MM-dd/HH"
      }
      bus {
        columns = ["date", "hour", "route_id"]
        pattern = "yyyy-MM-dd/HH"
      }
      incident {
        columns = ["date", "severity"]
        pattern = "yyyy-MM-dd"
      }
    }
  }

  # Spark Streaming Configuration
  streaming {
    # Trigger intervals
    trigger {
      traffic-aggregation = "30 seconds"
      bus-etl = "15 seconds"
      incident-enrichment = "10 seconds"
      data-lake = "1 minute"
      analytics = "30 seconds"
    }
    
    # Watermark settings for late data
    watermark {
      traffic = "5 minutes"
      bus = "2 minutes"
      incident = "1 minute"
    }
    
    # Window durations
    windows {
      short = "5 minutes"
      medium = "15 minutes"
      long = "30 minutes"
    }
    
    # State store configuration
    state-store {
      checkpoint-location = ${cityflow.delta-lake.checkpoint-location}
    }
  }

  # Data Quality Configuration
  data-quality {
    enabled = true
    
    validation {
      null-threshold = 0.1      # Max 10% null values
      schema-evolution = true
      duplicate-check = true
    }
    
    dlq {
      enabled = true
      topic = ${cityflow.kafka.topics.dead-letter-queue}
    }
  }

  # Geospatial Configuration
  geospatial {
    coordinate-precision = 6    # Decimal places for lat/lon
    distance-threshold = 100    # meters for route matching
    
    bounding-box {
      # Prishtina, Kosovo approximate bounds
      min-lat = 42.60
      max-lat = 42.70
      min-lon = 21.10
      max-lon = 21.20
    }
  }

  # Logging
  logging {
    level = "INFO"
    level = ${?LOG_LEVEL}
    
    metrics-interval = "30 seconds"
  }
}
